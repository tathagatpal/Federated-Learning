{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1234b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import syft as sy\n",
    "import copy\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from torch import Tensor\n",
    "from typing import List, Optional\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import importlib\n",
    "importlib.import_module('FLDataset')\n",
    "from FLDataset import load_dataset, getActualImgs\n",
    "from utils import averageModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0679460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to do updates according to Adam's Formula\n",
    "def adam(params: List[Tensor],grads: List[Tensor],exp_avgs: List[Tensor],exp_avg_sqs: List[Tensor],\n",
    "         max_exp_avg_sqs: List[Tensor],state_steps: List[int],*,beta1: float,beta2: float,lr: float,\n",
    "         eps: float):\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        grad = grads[i]\n",
    "        exp_avg = exp_avgs[i]\n",
    "        exp_avg_sq = exp_avg_sqs[i]\n",
    "        step = state_steps[i]\n",
    "\n",
    "        bias_correction1 = 1 - beta1 ** step\n",
    "        bias_correction2 = 1 - beta2 ** step\n",
    "\n",
    "        #Configuring the first and second moment running average coefficient\n",
    "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
    "        \n",
    "        denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
    "\n",
    "        step_size = lr / bias_correction1\n",
    "\n",
    "        param.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "#Creating Adam Optimizer which overrides the step function.\n",
    "class Adam(Optimizer):\n",
    "\n",
    "    #Constructor with 4 parameters: LR, Beta rate and epsilon\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-7):\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
    "        \n",
    "        #Calling constructor of in-built PyTorch\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    #THIS IS THE PART WHERE WE ARE IMPLEMENTING THE ADAM OPTIMIZER. \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            #Creating list to store various model parameters\n",
    "            params_with_grad = []\n",
    "            grads = []\n",
    "            exp_avgs = []\n",
    "            exp_avg_sqs = []\n",
    "            max_exp_avg_sqs = []\n",
    "            state_steps = []\n",
    "            beta1, beta2 = group['betas']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    grads.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    if len(state) == 0:\n",
    "                        state['step'] = 0\n",
    "                        #Gradient values' exponential moving average\n",
    "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        \n",
    "                        #Squared gradient values' exponential moving average\n",
    "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                       \n",
    "                    exp_avgs.append(state['exp_avg'])\n",
    "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
    "\n",
    "                    #Updating the number of steps for each param group\n",
    "                    state['step'] += 1\n",
    "                    \n",
    "                    #Store and append the step after update\n",
    "                    state_steps.append(state['step'])\n",
    "\n",
    "            #Calling Adam's function to update parameters\n",
    "            adam(params_with_grad, grads, exp_avgs,exp_avg_sqs,max_exp_avg_sqs,state_steps,\n",
    "                   beta1=beta1,beta2=beta2,lr=group['lr'],eps=group['eps'])\n",
    "            \n",
    "        #return value of loss   \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d380ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to define FL parameters\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.images = 60000\n",
    "        self.clients = 5\n",
    "        self.rounds = 100\n",
    "        self.epochs = 5\n",
    "        self.local_batches = 128\n",
    "        self.lr = 0.01\n",
    "        self.C = 0.9\n",
    "        self.drop_rate = 0.1\n",
    "        self.torch_seed = 0\n",
    "        self.log_interval = 10\n",
    "        self.iid = 'iid'\n",
    "        self.split_size = int(self.images / self.clients)\n",
    "        self.samples = self.split_size / self.images \n",
    "        self.use_cuda = False\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "# use_cuda = args.use_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b20084c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "## creating clients for FL\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "clients = []\n",
    "\n",
    "for i in range(args.clients):\n",
    "    clients.append({'hook': sy.VirtualWorker(hook, id=\"client{}\".format(i+1))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a2d0a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "tar: Error opening archive: Failed to open 'FashionMNIST.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "# Download FashionMNIST manually using 'wget' then uncompress the file\n",
    "!wget www.di.ens.fr/~lelarge/FashionMNIST.tar.gz\n",
    "!tar -zxvf FashionMNIST.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58bed796",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_train, global_test, train_group, test_group = load_dataset(args.clients, args.iid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d445700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inx, client in enumerate(clients):\n",
    "    trainset_ind_list = list(train_group[inx])\n",
    "    client['trainset'] = getActualImgs(global_train, trainset_ind_list, args.local_batches)\n",
    "    client['testset'] = getActualImgs(global_test, list(test_group[inx]), args.local_batches)\n",
    "    client['samples'] = len(trainset_ind_list) / args.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1512fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "global_test_dataset = datasets.FashionMNIST('./', train=False, download=True, transform=transform)\n",
    "global_test_loader = DataLoader(global_test_dataset, batch_size=args.local_batches, shuffle=True)\n",
    "\n",
    "global_train_dataset = datasets.FashionMNIST('./', train=True, download=True, transform=transform)\n",
    "global_train_loader = DataLoader(global_train_dataset, batch_size=args.local_batches, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8e0c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating neural net structure using pytorch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51658f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to do updates during FL model rounds (calls step function)\n",
    "\n",
    "def ClientUpdate(args, device, client):\n",
    "    client['model'].train()\n",
    "    client['model'].send(client['hook'])\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for batch_idx, (data, target) in enumerate(client['trainset']):\n",
    "            data = data.send(client['hook'])\n",
    "            target = target.send(client['hook'])\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            client['optim'].zero_grad()\n",
    "            output = client['model'](data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            client['optim'].step()\n",
    "            \n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                loss = loss.get() \n",
    "                print('Model {} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    client['hook'].id,\n",
    "                    epoch, batch_idx * args.local_batches, len(client['trainset']) * args.local_batches, \n",
    "                    100. * batch_idx / len(client['trainset']), loss))\n",
    "                \n",
    "    client['model'].get() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b077712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prints values periodically on testing data\n",
    "def test(args, model, device, test_loader, name, isPrint):\n",
    "\n",
    "    model.eval()   \n",
    "    test_losss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_losss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_losss /= len(test_loader.dataset)\n",
    "    \n",
    "    if isPrint:\n",
    "        print('\\nTest set: Average loss for {} model: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        name, test_losss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        test_loss.append(test_losss)\n",
    "        test_acc.append(correct/len(test_loader.dataset))\n",
    "        \n",
    "    else:\n",
    "        train_loss.append(test_losss)\n",
    "        train_acc.append(correct/len(test_loader.dataset))\n",
    "   \n",
    "        \n",
    "    #return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95e485cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model client2 Train Epoch: 1 [0/12032 (0%)]\tLoss: 2.303589\n",
      "Model client2 Train Epoch: 1 [1280/12032 (11%)]\tLoss: 0.971004\n",
      "Model client2 Train Epoch: 1 [2560/12032 (21%)]\tLoss: 0.765004\n",
      "Model client2 Train Epoch: 1 [3840/12032 (32%)]\tLoss: 0.761951\n",
      "Model client2 Train Epoch: 1 [5120/12032 (43%)]\tLoss: 0.653680\n",
      "Model client2 Train Epoch: 1 [6400/12032 (53%)]\tLoss: 0.787424\n",
      "Model client2 Train Epoch: 1 [7680/12032 (64%)]\tLoss: 0.591428\n",
      "Model client2 Train Epoch: 1 [8960/12032 (74%)]\tLoss: 0.712314\n",
      "Model client2 Train Epoch: 1 [10240/12032 (85%)]\tLoss: 0.703854\n",
      "Model client2 Train Epoch: 1 [11520/12032 (96%)]\tLoss: 0.633102\n",
      "Model client4 Train Epoch: 1 [0/12032 (0%)]\tLoss: 2.324175\n",
      "Model client4 Train Epoch: 1 [1280/12032 (11%)]\tLoss: 1.173134\n",
      "Model client4 Train Epoch: 1 [2560/12032 (21%)]\tLoss: 1.042286\n",
      "Model client4 Train Epoch: 1 [3840/12032 (32%)]\tLoss: 0.697876\n",
      "Model client4 Train Epoch: 1 [5120/12032 (43%)]\tLoss: 0.774610\n",
      "Model client4 Train Epoch: 1 [6400/12032 (53%)]\tLoss: 0.626434\n",
      "Model client4 Train Epoch: 1 [7680/12032 (64%)]\tLoss: 0.570076\n",
      "Model client4 Train Epoch: 1 [8960/12032 (74%)]\tLoss: 0.685762\n",
      "Model client4 Train Epoch: 1 [10240/12032 (85%)]\tLoss: 0.584962\n",
      "Model client4 Train Epoch: 1 [11520/12032 (96%)]\tLoss: 0.474337\n",
      "Model client1 Train Epoch: 1 [0/12032 (0%)]\tLoss: 2.335738\n",
      "Model client1 Train Epoch: 1 [1280/12032 (11%)]\tLoss: 0.884988\n",
      "Model client1 Train Epoch: 1 [2560/12032 (21%)]\tLoss: 0.796344\n",
      "Model client1 Train Epoch: 1 [3840/12032 (32%)]\tLoss: 0.791739\n",
      "Model client1 Train Epoch: 1 [5120/12032 (43%)]\tLoss: 0.727011\n",
      "Model client1 Train Epoch: 1 [6400/12032 (53%)]\tLoss: 0.602553\n",
      "Model client1 Train Epoch: 1 [7680/12032 (64%)]\tLoss: 0.621779\n",
      "Model client1 Train Epoch: 1 [8960/12032 (74%)]\tLoss: 0.491982\n",
      "Model client1 Train Epoch: 1 [10240/12032 (85%)]\tLoss: 0.628887\n",
      "Model client1 Train Epoch: 1 [11520/12032 (96%)]\tLoss: 0.543455\n",
      "\n",
      "Test set: Average loss for Global model: 1.5871, Accuracy: 7907/10000 (79%)\n",
      "\n",
      "Model client1 Train Epoch: 1 [0/12032 (0%)]\tLoss: 1.626275\n",
      "Model client1 Train Epoch: 1 [1280/12032 (11%)]\tLoss: 0.856498\n",
      "Model client1 Train Epoch: 1 [2560/12032 (21%)]\tLoss: 0.640269\n",
      "Model client1 Train Epoch: 1 [3840/12032 (32%)]\tLoss: 0.713606\n",
      "Model client1 Train Epoch: 1 [5120/12032 (43%)]\tLoss: 0.574820\n",
      "Model client1 Train Epoch: 1 [6400/12032 (53%)]\tLoss: 0.616791\n",
      "Model client1 Train Epoch: 1 [7680/12032 (64%)]\tLoss: 0.586191\n",
      "Model client1 Train Epoch: 1 [8960/12032 (74%)]\tLoss: 0.669190\n",
      "Model client1 Train Epoch: 1 [10240/12032 (85%)]\tLoss: 0.426058\n",
      "Model client1 Train Epoch: 1 [11520/12032 (96%)]\tLoss: 0.537344\n",
      "Model client5 Train Epoch: 1 [0/12032 (0%)]\tLoss: 1.584208\n",
      "Model client5 Train Epoch: 1 [1280/12032 (11%)]\tLoss: 0.774768\n",
      "Model client5 Train Epoch: 1 [2560/12032 (21%)]\tLoss: 0.558836\n",
      "Model client5 Train Epoch: 1 [3840/12032 (32%)]\tLoss: 0.675393\n",
      "Model client5 Train Epoch: 1 [5120/12032 (43%)]\tLoss: 0.695546\n",
      "Model client5 Train Epoch: 1 [6400/12032 (53%)]\tLoss: 0.581131\n",
      "Model client5 Train Epoch: 1 [7680/12032 (64%)]\tLoss: 0.459884\n",
      "Model client5 Train Epoch: 1 [8960/12032 (74%)]\tLoss: 0.487209\n",
      "Model client5 Train Epoch: 1 [10240/12032 (85%)]\tLoss: 0.516824\n",
      "Model client5 Train Epoch: 1 [11520/12032 (96%)]\tLoss: 0.480139\n",
      "Model client3 Train Epoch: 1 [0/12032 (0%)]\tLoss: 1.609092\n",
      "Model client3 Train Epoch: 1 [1280/12032 (11%)]\tLoss: 0.670478\n",
      "Model client3 Train Epoch: 1 [2560/12032 (21%)]\tLoss: 0.608029\n",
      "Model client3 Train Epoch: 1 [3840/12032 (32%)]\tLoss: 0.583844\n",
      "Model client3 Train Epoch: 1 [5120/12032 (43%)]\tLoss: 0.633294\n",
      "Model client3 Train Epoch: 1 [6400/12032 (53%)]\tLoss: 0.501374\n",
      "Model client3 Train Epoch: 1 [7680/12032 (64%)]\tLoss: 0.481323\n",
      "Model client3 Train Epoch: 1 [8960/12032 (74%)]\tLoss: 0.619566\n",
      "Model client3 Train Epoch: 1 [10240/12032 (85%)]\tLoss: 0.458415\n",
      "Model client3 Train Epoch: 1 [11520/12032 (96%)]\tLoss: 0.564730\n",
      "\n",
      "Test set: Average loss for Global model: 1.5073, Accuracy: 8134/10000 (81%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.torch_seed)\n",
    "global_model = Net()\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for client in clients:\n",
    "    torch.manual_seed(args.torch_seed)\n",
    "    client['model'] = Net().to(device)\n",
    "    client['optim'] = Adam(client['model'].parameters())\n",
    "    \n",
    "for fed_round in range(args.rounds):\n",
    "    \n",
    "    # number of selected clients\n",
    "    m = int(max(args.C * args.clients, 1))\n",
    "\n",
    "    # Selected devices\n",
    "    np.random.seed(fed_round)\n",
    "    selected_clients_inds = np.random.choice(range(len(clients)), m, replace=False)\n",
    "    selected_clients = [clients[i] for i in selected_clients_inds]\n",
    "    \n",
    "    # Active devices\n",
    "    np.random.seed(fed_round)\n",
    "    active_clients_inds = np.random.choice(selected_clients_inds, int((1-args.drop_rate) * m), replace=False)\n",
    "    active_clients = [clients[i] for i in active_clients_inds]\n",
    "    \n",
    "    # Training \n",
    "    for client in active_clients:\n",
    "        ClientUpdate(args, device, client)\n",
    "        \n",
    "    # Averaging \n",
    "    global_model = averageModels(global_model, active_clients)\n",
    "    \n",
    "    # Testing the average model\n",
    "    test(args, global_model, device, global_test_loader, 'Global',True)\n",
    "    test(args, global_model, device, global_train_loader, 'Global',False)\n",
    "    # Share the global model with the clients\n",
    "    for client in clients:\n",
    "        client['model'].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "if (args.save_model):\n",
    "    torch.save(global_model.state_dict(), \"FedAvg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3cbc0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.581006516265869, 1.500590850830078],\n",
       " [1.5870809768676757, 1.5072730680465698])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d66ab009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79.07 81.34]\n"
     ]
    }
   ],
   "source": [
    "print(np.multiply(test_acc,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09191414",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(i+1) for i in range(len(train_loss))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(x,train_loss)\n",
    "#plt.plot(x,test_loss)\n",
    "plt.plot(x,train_acc)\n",
    "plt.plot(x,test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc843e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
